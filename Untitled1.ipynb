{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['データ内連番', '球種', '投球位置区域', '年度', '試合ID', '試合内連番', '試合内投球数', '日付', '時刻',\n",
      "       'ホームチームID', 'アウェイチームID', '球場ID', '球場名', '試合種別詳細', 'イニング', '表裏',\n",
      "       'イニング内打席数', '打席内投球数', '投手ID', '投手チームID', '投手投球左右', '投手役割', '投手登板順',\n",
      "       '投手試合内対戦打者数', '投手試合内投球数', '投手イニング内投球数', '打者ID', '打者チームID', '打者打席左右',\n",
      "       '打者打順', '打者守備位置', '打者試合内打席数', 'プレイ前ホームチーム得点数', 'プレイ前アウェイチーム得点数',\n",
      "       'プレイ前アウト数', 'プレイ前ボール数', 'プレイ前ストライク数', 'プレイ前走者状況', '一塁走者ID', '二塁走者ID',\n",
      "       '三塁走者ID', '捕手ID', '一塁手ID', '二塁手ID', '三塁手ID', '遊撃手ID', '左翼手ID', '中堅手ID',\n",
      "       '右翼手ID', '成績対象投手ID', '成績対象打者ID'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    120396\n",
       "2     47774\n",
       "4     21344\n",
       "1     19213\n",
       "3     18161\n",
       "5     13940\n",
       "7     13368\n",
       "6      2921\n",
       "Name: 球種, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "train_pitch_df = pd.read_csv(\"../Data/train_pitch.csv\")\n",
    "train_player_df = pd.read_csv(\"../Data/train_player.csv\")\n",
    "\n",
    "print(train_pitch_df.columns)\n",
    "train_pitch_df.head()\n",
    "\n",
    "#one-hot化が必要\n",
    "\n",
    "columns1 = ['球場名','試合種別詳細','イニング','表裏','打者打席左右',\n",
    "            '投手投球左右','投手役割','投手登板順','打者打順', '打者守備位置']\n",
    "\n",
    "#one-hot化が必要だが多いので保留\n",
    "columns2 = ['日付', '時刻','年度','試合ID','投手ID','投手チームID','打者ID','打者チームID','プレイ前走者状況',\n",
    "            '一塁走者ID', '二塁走者ID','三塁走者ID', '捕手ID', '一塁手ID', '二塁手ID', '三塁手ID', '遊撃手ID',\n",
    "            '左翼手ID', '中堅手ID','右翼手ID', '成績対象投手ID', '成績対象打者ID','ホームチームID','アウェイチームID','球場ID']\n",
    "\n",
    "#one-hot化が必要なし\n",
    "columns3 = ['試合内連番', '試合内投球数','イニング内打席数', '打席内投球数','投手試合内対戦打者数', '投手試合内投球数',\n",
    "            '投手イニング内投球数','打者試合内打席数', 'プレイ前ホームチーム得点数', 'プレイ前アウェイチーム得点数',\n",
    "            'プレイ前アウト数', 'プレイ前ボール数', 'プレイ前ストライク数']\n",
    "\n",
    "columns4 = ['データ内連番','球種','投球位置区域']\n",
    "\n",
    "key = ['データ内連番']\n",
    "y1 = ['球種'] #目的変数１\n",
    "y2 = ['投球位置区域'] #目的変数２\n",
    "\n",
    "#読み込み\n",
    "train_pitch_df = pd.read_csv(\"../Data/train_pitch.csv\")\n",
    "\n",
    "#指定cloumnのone-hot化\n",
    "train_pitch_df = pd.get_dummies(train_pitch_df, columns = columns1)\n",
    "\n",
    "#余分な特徴量を削除\n",
    "train_pitch_df2 = train_pitch_df.drop(columns = columns2)\n",
    "X = train_pitch_df2.drop(columns = columns4)\n",
    "\n",
    "Y = train_pitch_df['球種']\n",
    "Y2 = train_pitch_df['球種'].astype(str)\n",
    "Y2 = pd.get_dummies(Y2)\n",
    "\n",
    "X.head()\n",
    "\n",
    "#要素ごとの個数を表示\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "2921\n",
      "10000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#要素の偏りをなくしたデータセットの作成\n",
    "\n",
    "Y_X = pd.concat([Y,X],axis = 1)\n",
    "Y_X0 = Y_X[Y_X['球種'] == 0]\n",
    "Y_X_10000 = pd.DataFrame()\n",
    "for i in range(8):\n",
    "    tmp = Y_X[Y_X['球種'] == i]\n",
    "    if len(tmp) > 10000:\n",
    "        tmp = tmp[:10000]\n",
    "    else:\n",
    "        tmp = tmp[:len(tmp)]\n",
    "    print(len(tmp))\n",
    "    Y_X_10000 = pd.concat([Y_X_10000, tmp])\n",
    "\n",
    "len(Y_X_10000)\n",
    "X2 = Y_X_10000.drop(columns = [\"球種\"])\n",
    "Y2 = Y_X_10000[\"球種\"]\n",
    "\n",
    "#相関係数\n",
    "Y_X_corr = Y_X.corr()\n",
    "print(type(Y_X_corr))\n",
    "\n",
    "#相関係数のソート\n",
    "#200行まで表示\n",
    "pd.set_option('display.max_rows',200)\n",
    "Y_X_corr_rank = Y_X_corr['球種']\n",
    "Y_X_corr_rank = Y_X_corr.sort_values('球種',ascending=False)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Optimization\n",
      "---------------------------------\n",
      "Inital_Design:1 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.352  max_depth = 7.931  min_child_weight = 0.291  subsample = 0.886  colsample_bytree = 0.964  gamma = 7.647  n_estimators = 171.357  learning_rate = 0.263  reg_lambda = 0.943  reg_alpha = 0.206  \n",
      "\n",
      "Logloss: 1.500462\n",
      "---------------------------------\n",
      "Inital_Design:2 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.391  max_depth = 7.916  min_child_weight = 1.581  subsample = 0.937  colsample_bytree = 1.000  gamma = 7.628  n_estimators = 133.910  learning_rate = 0.175  reg_lambda = 0.975  reg_alpha = 0.095  \n",
      "\n",
      "Logloss: 1.502072\n",
      "---------------------------------\n",
      "Inital_Design:3 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.351  max_depth = 12.567  min_child_weight = 0.879  subsample = 0.882  colsample_bytree = 0.809  gamma = 6.407  n_estimators = 148.721  learning_rate = 0.737  reg_lambda = 1.000  reg_alpha = 0.231  \n",
      "\n",
      "Logloss: 1.496881\n",
      "---------------------------------\n",
      "Inital_Design:4 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.350  max_depth = 10.964  min_child_weight = 0.082  subsample = 0.829  colsample_bytree = 0.891  gamma = 1.629  n_estimators = 181.460  learning_rate = 0.450  reg_lambda = 0.974  reg_alpha = 0.110  \n",
      "\n",
      "Logloss: 1.457839\n",
      "---------------------------------\n",
      "Inital_Design:5 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.338  max_depth = 6.215  min_child_weight = 0.298  subsample = 0.974  colsample_bytree = 0.915  gamma = 4.791  n_estimators = 37.317  learning_rate = 0.979  reg_lambda = 0.888  reg_alpha = 0.233  \n",
      "\n",
      "Logloss: 1.497312\n",
      "---------------------------------\n",
      "Inital_Design:6 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.371  max_depth = 5.875  min_child_weight = 1.416  subsample = 0.837  colsample_bytree = 0.999  gamma = 4.133  n_estimators = 90.632  learning_rate = 0.309  reg_lambda = 0.940  reg_alpha = 0.294  \n",
      "\n",
      "Logloss: 1.492764\n",
      "---------------------------------\n",
      "Inital_Design:7 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.325  max_depth = 3.913  min_child_weight = 1.268  subsample = 0.933  colsample_bytree = 0.859  gamma = 5.461  n_estimators = 70.023  learning_rate = 0.342  reg_lambda = 0.895  reg_alpha = 0.136  \n",
      "\n",
      "Logloss: 1.514988\n",
      "---------------------------------\n",
      "Inital_Design:8 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.305  max_depth = 7.492  min_child_weight = 1.919  subsample = 0.972  colsample_bytree = 0.864  gamma = 4.764  n_estimators = 58.025  learning_rate = 0.507  reg_lambda = 0.983  reg_alpha = 0.040  \n",
      "\n",
      "Logloss: 1.489811\n",
      "---------------------------------\n",
      "Inital_Design:9 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.358  max_depth = 13.131  min_child_weight = 1.814  subsample = 0.934  colsample_bytree = 0.856  gamma = 9.022  n_estimators = 100.715  learning_rate = 0.351  reg_lambda = 0.845  reg_alpha = 0.068  \n",
      "\n",
      "Logloss: 1.503495\n",
      "---------------------------------\n",
      "Inital_Design:10 / 10  Try:0 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.375  max_depth = 12.857  min_child_weight = 0.114  subsample = 0.903  colsample_bytree = 0.870  gamma = 1.603  n_estimators = 187.696  learning_rate = 0.662  reg_lambda = 0.978  reg_alpha = 0.274  \n",
      "\n",
      "Logloss: 1.480692\n",
      "---------------------------------\n",
      "Inital_Design:10 / 10  Try:1 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.340  max_depth = 11.098  min_child_weight = 0.015  subsample = 0.800  colsample_bytree = 0.879  gamma = 0.049  n_estimators = 181.932  learning_rate = 0.424  reg_lambda = 0.980  reg_alpha = 0.027  \n",
      "\n",
      "Logloss: 1.696153\n",
      "---------------------------------\n",
      "Inital_Design:10 / 10  Try:2 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.374  max_depth = 12.967  min_child_weight = 0.066  subsample = 0.891  colsample_bytree = 0.852  gamma = 1.533  n_estimators = 187.773  learning_rate = 0.602  reg_lambda = 0.979  reg_alpha = 0.296  \n",
      "\n",
      "Logloss: 1.475170\n",
      "---------------------------------\n",
      "Inital_Design:10 / 10  Try:3 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.375  max_depth = 13.276  min_child_weight = 0.066  subsample = 0.879  colsample_bytree = 0.848  gamma = 1.940  n_estimators = 188.337  learning_rate = 0.361  reg_lambda = 0.958  reg_alpha = 0.300  \n",
      "\n",
      "Logloss: 1.454235\n",
      "---------------------------------\n",
      "Inital_Design:10 / 10  Try:4 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.358  max_depth = 13.131  min_child_weight = 0.173  subsample = 0.854  colsample_bytree = 0.802  gamma = 1.456  n_estimators = 188.618  learning_rate = 0.112  reg_lambda = 0.954  reg_alpha = 0.295  \n",
      "\n",
      "Logloss: 1.434684\n",
      "---------------------------------\n",
      "Inital_Design:10 / 10  Try:5 / 20\n",
      "\n",
      "Next bounds is\n",
      "eta = 0.362  max_depth = 13.696  min_child_weight = 0.000  subsample = 0.812  colsample_bytree = 0.800  gamma = 1.293  n_estimators = 188.733  learning_rate = 0.001  reg_lambda = 0.965  reg_alpha = 0.300  "
     ]
    }
   ],
   "source": [
    "########### XGBoost ###########\n",
    "import xgboost as xgb\n",
    "import GPy\n",
    "import GPyOpt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "'''\n",
    "X：特徴量\n",
    "Y：目的変数\n",
    "必要なもの\n",
    "pip install xgboost\n",
    "pip install japanize-matplotlib\n",
    "'''\n",
    "#通常\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.3, shuffle=True)\n",
    "\n",
    "#球種割合揃え\n",
    "#train_x, test_x, train_y, test_y = train_test_split(X2, Y2, test_size=0.3, shuffle=True)\n",
    "\n",
    "# #専用のデータ形式に変形\n",
    "# dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "# dtest = xgb.DMatrix(test_x, label=test_y)\n",
    "max_iter = 20\n",
    "init_num = 10\n",
    "\n",
    "initial_design = 1\n",
    "try_ = 0\n",
    "bounds_list = list()\n",
    "\n",
    "def f(x):\n",
    "    model = xgb.XGBClassifier(num_class = 8, \n",
    "                              eta = float(x[:,0]),\n",
    "                              max_depth = int(x[:,1]),\n",
    "                              min_child_weight = float(x[:,2]),\n",
    "                              subsample = float(x[:,3]),\n",
    "                              colsample_bytree = float(x[:,4]),\n",
    "                              gamma = float(x[:,5]),\n",
    "                              n_estimators = int(x[:,6]),\n",
    "                              learning_rate = float(x[:,7]),\n",
    "                              reg_lambda = float(x[:,8]),\n",
    "                              reg_alpha = float(x[:,9]),\n",
    "                              tree_method = 'gpu_hist',\n",
    "                              objective = 'multi:softprob'\n",
    "                              )\n",
    "    global initial_design\n",
    "    global try_\n",
    "    global bounds_list\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"Inital_Design:{0} / {1}  Try:{2} / {3}\\n\".format(initial_design, init_num, try_, max_iter))\n",
    "    \n",
    "    if initial_design < init_num:\n",
    "        initial_design += 1\n",
    "    else:\n",
    "        try_ += 1\n",
    "        \n",
    "    print(\"Next bounds is\")\n",
    "    x = np.reshape(x,(x.size,)) #nazeka(1,10)nonizigennninatteru\n",
    "    for bound, x_ in zip(bounds, x):\n",
    "        print(f\"{bound['name']:s} = {x_:.3f}  \", end=\"\")\n",
    "    \n",
    "    \n",
    "    x_list = x.tolist() #numpywolistka(numpyhaappendgaossoitame)\n",
    "    bounds_list.append(x_list)\n",
    "    \n",
    "    # CV\n",
    "    kfold = KFold(n_splits=5, random_state=7)\n",
    "    results = cross_validate(model, train_x, train_y, scoring = 'neg_log_loss', cv=kfold)\n",
    "    \n",
    "    score = results['test_score'].mean()*(-1)\n",
    "    print(f\"\\n\\nLogloss: {score:f}\")\n",
    "    \n",
    "    \n",
    "    return score\n",
    "\n",
    "bounds = [{'name': 'eta', 'type': 'continuous', 'domain': (0.3,0.4)},\n",
    "          {'name': 'max_depth', 'type': 'continuous', 'domain': (3,15)},\n",
    "          {'name': 'min_child_weight', 'type': 'continuous', 'domain': (0,2)},\n",
    "          {'name': 'subsample', 'type': 'continuous', 'domain': (0.8,1)},\n",
    "          {'name': 'colsample_bytree', 'type': 'continuous', 'domain': (0.8,1)},\n",
    "          {'name': 'gamma', 'type': 'continuous', 'domain': (0,10)},\n",
    "          {'name': 'n_estimators', 'type': 'continuous', 'domain': (10,200)},\n",
    "          {'name': 'learning_rate', 'type': 'continuous', 'domain': (0,1)},\n",
    "          {'name': 'reg_lambda', 'type': 'continuous', 'domain': (0.8,1)},\n",
    "          {'name': 'reg_alpha', 'type': 'continuous', 'domain': (0,0.3)}]\n",
    "\n",
    "print(\"Bayesian Optimization\")\n",
    "#initial_design_numdata　\n",
    "myBopt = GPyOpt.methods.BayesianOptimization(f=f,initial_design_numdata=init_num, verbosity = True, domain=bounds)\n",
    "myBopt.run_optimization(max_iter=max_iter)\n",
    "print(\"End\")\n",
    "result_z = myBopt.Y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(result_z)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"CV Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "#最適なパラメータの表示\n",
    "print(\"### best parameters ###\")\n",
    "\n",
    "for bound, x_opt in zip(bounds, myBopt.x_opt):\n",
    "    print(f\"{bound['name']:s} = {x_opt:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0d195047eec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m Xgb = xgb.XGBClassifier(num_class=10, eta=x[0], max_depth=int(x[1]),min_child_weight=x[2],\n\u001b[1;32m      4\u001b[0m                         \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         learning_rate=x[7], reg_lambda=x[8], reg_alpha=x[9],num_boost_round=int(x[10]))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mXgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "x = myBopt.x_opt\n",
    "\n",
    "Xgb = xgb.XGBClassifier(num_class=10, eta=x[0], max_depth=int(x[1]),min_child_weight=x[2],\n",
    "                        subsample=x[3], colsample_bytree=x[4], gamma=x[5], n_estimators=int(x[6]),\n",
    "                        learning_rate=x[7], reg_lambda=x[8], reg_alpha=x[9])\n",
    "Xgb.fit(train_x,train_y)\n",
    "\n",
    "#最適なパラメータで再度学習、結果表示\n",
    "xgb_predict = Xgb.predict(test_x)\n",
    "print(\"Optimized XGBoost\")\n",
    "print(accuracy_score(test_y, xgb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:18:15] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:18:24] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:18:34] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:18:43] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:18:53] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Logloss: 1.458372\n"
     ]
    }
   ],
   "source": [
    "dtest = xgb.DMatrix(test_x, label=test_y)  \n",
    "evallist = [(dtest, 'eval')] \n",
    "model = xgb.XGBClassifier(num_class = 8,\n",
    "                          tree_method = 'gpu_hist',\n",
    "                          objective = 'multi:softprob',\n",
    "                          \n",
    "                          )\n",
    "\n",
    "# CV\n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "results = cross_validate(model, train_x, train_y, scoring = 'neg_log_loss', cv=kfold)\n",
    "score = results['test_score'].mean()*(-1)\n",
    "print(f\"Logloss: {score:f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
